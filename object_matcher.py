"""
ML —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∏—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–∞ –∫–∞—Ä—Ç–∞—Ö
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç YOLOv8 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
"""
import cv2
import numpy as np
from typing import Tuple, Optional, Dict, List
import warnings

warnings.filterwarnings('ignore')

try:
    from ultralytics import YOLO
    import torch
    import torch.nn as nn
    import torchvision.transforms as transforms
    from torchvision.models import mobilenet_v2, MobileNet_V2_Weights
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("YOLO/PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ultralytics torch torchvision")


class ObjectDescriptor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –∫–∞—Ä—Ç–µ"""
    
    def __init__(self, bbox: Tuple[int, int, int, int], 
                 class_id: int, confidence: float, 
                 embedding: np.ndarray, image_patch: np.ndarray):
        """
        Args:
            bbox: (x1, y1, x2, y2) –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box
            class_id: ID –∫–ª–∞—Å—Å–∞ –æ–±—ä–µ–∫—Ç–∞
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏
            embedding: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞
            image_patch: –ü–∞—Ç—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–±—ä–µ–∫—Ç–æ–º
        """
        self.bbox = bbox
        self.class_id = class_id
        self.confidence = confidence
        self.embedding = embedding
        self.image_patch = image_patch
        self.center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)
        self.area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
    
    def distance(self, other: 'ObjectDescriptor') -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏"""
        return 1 - np.dot(self.embedding, other.embedding) / (
            np.linalg.norm(self.embedding) * np.linalg.norm(other.embedding) + 1e-8
        )


class ObjectMatcher:
    """
    ML —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∫–∞—Ä—Ç–µ –ø–æ –æ–±—ä–µ–∫—Ç–∞–º
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç YOLOv8 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    """
    
    def __init__(self, model_size: str = 'n', confidence_threshold: float = 0.25):
        """
        Args:
            model_size: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ YOLO ('n', 's', 'm', 'l', 'x')
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        self.confidence_threshold = confidence_threshold
        self.model_size = model_size
        self.yolo_available = YOLO_AVAILABLE
        self.yolo_model = None
        self.feature_extractor = None
        self.device = None
        self.transform = None
        
        if self.yolo_available:
            self._init_models()
        else:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: YOLO –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥.")
    
    def _init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ YOLO –∏ SentenceTransformers"""
        try:
            print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–µ–π...")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º YOLOv8
            model_path = f'yolov8{self.model_size}.pt'
            self.yolo_model = YOLO(model_path)
            print(f"‚úì YOLOv8-{self.model_size} –∑–∞–≥—Ä—É–∂–µ–Ω")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–∞—Ç—á–µ–π
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º MobileNetV2 –∫–∞–∫ –ª–µ–≥–∫–∏–π feature extractor
            weights = MobileNet_V2_Weights.IMAGENET1K_V1
            mobilenet = mobilenet_v2(weights=weights)
            # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π
            self.feature_extractor = nn.Sequential(*list(mobilenet.children())[:-1])
            self.feature_extractor.eval()
            self.feature_extractor.to(self.device)
            print("‚úì MobileNetV2 –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
            
            print("‚úì ML –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.yolo_available = False
    
    def detect_objects(self, image: np.ndarray) -> List[ObjectDescriptor]:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (BGR)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤
        """
        if not self.yolo_available or self.yolo_model is None:
            return []
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB –¥–ª—è YOLO
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã
        results = self.yolo_model(image_rgb, 
                                 conf=self.confidence_threshold,
                                 verbose=False)
        
        objects = []
        if len(results) > 0 and results[0].boxes is not None:
            boxes = results[0].boxes
            for i in range(len(boxes)):
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy()
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                class_id = int(boxes.cls[i].cpu().numpy())
                confidence = float(boxes.conf[i].cpu().numpy())
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                patch = image[y1:y2, x1:x2]
                if patch.size == 0:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–∞—Ç—á–∞
                patch_features = self._extract_visual_features(patch)
                
                objects.append(ObjectDescriptor(
                    bbox=(x1, y1, x2, y2),
                    class_id=class_id,
                    confidence=confidence,
                    embedding=patch_features,
                    image_patch=patch
                ))
        
        return objects
    
    def _extract_visual_features(self, patch: np.ndarray) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ø–∞—Ç—á–∞
        
        Args:
            patch: –ü–∞—Ç—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = []
        
        # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ü–≤–µ—Ç–æ–≤ –≤ HSV
        hsv = cv2.cvtColor(patch, cv2.COLOR_BGR2HSV)
        hist_h = cv2.calcHist([hsv], [0], None, [32], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [32], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [32], [0, 256])
        
        # 2. –ö—Ä–∞–π (–≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–æ–≤)
        gray = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (patch.shape[0] * patch.shape[1])
        
        # 3. –¢–µ–∫—Å—Ç—É—Ä–∞ (–≤–∞—Ä–∏–∞—Ü–∏—è —è—Ä–∫–æ—Å—Ç–∏)
        texture_variance = np.var(gray)
        
        # 4. –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ MobileNet (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        if self.feature_extractor is not None and self.transform is not None:
            try:
                patch_rgb = cv2.cvtColor(patch, cv2.COLOR_BGR2RGB)
                patch_tensor = self.transform(patch_rgb).unsqueeze(0)
                
                with torch.no_grad():
                    features = self.feature_extractor(patch_tensor)
                    features = features.squeeze().cpu().numpy()
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 256 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    conv_features = features.flatten()[:256]
            except Exception as e:
                conv_features = np.zeros(256)
        else:
            conv_features = np.zeros(256)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = np.concatenate([
            hist_h.flatten(),
            hist_s.flatten(),
            hist_v.flatten(),
            [edge_density, texture_variance],
            conv_features
        ])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        features = features / (np.linalg.norm(features) + 1e-8)
        
        return features
    
    def find_location(self, large_map: np.ndarray, small_image: np.ndarray,
                     search_step: int = 500, top_k: int = 5) -> Optional[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –º–∞–ª–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–æ–π –∫–∞—Ä—Ç–µ
        
        Args:
            large_map: –ë–æ–ª—å—à–∞—è –∫–∞—Ä—Ç–∞
            small_image: –ú–∞–ª–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä—ã –¥—Ä–æ–Ω–∞
            search_step: –®–∞–≥ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç—ã (–¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–∞—Ä—Ç)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        """
        if not self.yolo_available:
            return None
        
        print(f"–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –º–∞–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏...")
        small_objects = self.detect_objects(small_image)
        
        if len(small_objects) == 0:
            print("‚ö† –û–±—ä–µ–∫—Ç—ã –Ω–∞ –º–∞–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π ORB –º–µ—Ç–æ–¥...")
            return self._find_location_fallback(large_map, small_image)
        
        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(small_objects)} –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –º–∞–ª–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
        
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –∫–∞—Ä—Ç —Å–∫–∞–Ω–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç—è–º
        map_h, map_w = large_map.shape[:2]
        small_h, small_w = small_image.shape[:2]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –∫–∞—Ä—Ç—ã
        if map_w > 5000 or map_h > 5000:
            result = self._search_in_large_map(large_map, small_objects, small_h, small_w, search_step)
        else:
            result = self._search_small_map(large_map, small_objects, small_h, small_w, top_k)
        
        # –ï—Å–ª–∏ YOLO –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º ORB
        if result is None:
            print("‚ö† YOLO –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π ORB –º–µ—Ç–æ–¥...")
            return self._find_location_fallback(large_map, small_image)
        
        return result
    
    def _find_location_fallback(self, large_map: np.ndarray, small_image: np.ndarray) -> Optional[Dict]:
        """
        –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ORB
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ YOLO –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—ä–µ–∫—Ç—ã
        """
        try:
            from image_matcher import ImageMatcher
            orb_matcher = ImageMatcher()
            return orb_matcher.find_location(large_map, small_image)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ ORB –º–µ—Ç–æ–¥–∞: {e}")
            return None
    
    def _search_small_map(self, large_map: np.ndarray, small_objects: List[ObjectDescriptor],
                         small_h: int, small_w: int, top_k: int) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π –∫–∞—Ä—Ç–µ (< 5000x5000)"""
        print("–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –±–æ–ª—å—à–æ–π –∫–∞—Ä—Ç–µ...")
        large_objects = self.detect_objects(large_map)
        
        if len(large_objects) == 0:
            print("–û–±—ä–µ–∫—Ç—ã –Ω–∞ –±–æ–ª—å—à–æ–π –∫–∞—Ä—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(large_objects)} –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –±–æ–ª—å—à–æ–π –∫–∞—Ä—Ç–µ")
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
        matches = self._find_object_matches(small_objects, large_objects, top_k)
        
        if len(matches) == 0:
            print("–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return None
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        return self._estimate_position(matches, large_map.shape, small_h, small_w)
    
    def _search_in_large_map(self, large_map: np.ndarray, small_objects: List[ObjectDescriptor],
                            small_h: int, small_w: int, search_step: int) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–∞ –±–æ–ª—å—à–æ–π –∫–∞—Ä—Ç–µ (>= 5000x5000) —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        print(f"–ë–æ–ª—å—à–∞—è –∫–∞—Ä—Ç–∞: {large_map.shape[1]}x{large_map.shape[0]}")
        print("–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫...")
        
        map_h, map_w = large_map.shape[:2]
        
        # –£—Ä–æ–≤–µ–Ω—å 1: –ì—Ä—É–±–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º —à–∞–≥–æ–º
        print("–£—Ä–æ–≤–µ–Ω—å 1: –ì—Ä—É–±–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        best_candidates = []
        
        for y in range(0, map_h - small_h, search_step * 3):
            for x in range(0, map_w - small_w, search_step * 3):
                region = large_map[y:min(y + small_h, map_h), 
                                 x:min(x + small_w, map_w)]
                
                region_objects = self.detect_objects(region)
                if len(region_objects) > 0:
                    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –±–æ–ª—å—à—É—é –∫–∞—Ä—Ç—É
                    for obj in region_objects:
                        obj.bbox = (obj.bbox[0] + x, obj.bbox[1] + y,
                                   obj.bbox[2] + x, obj.bbox[3] + y)
                        obj.center = (obj.center[0] + x, obj.center[1] + y)
                    
                    matches = self._find_object_matches(small_objects, region_objects, top_k=3)
                    if len(matches) > 0:
                        confidence = sum(m.distance for m in matches) / len(matches)
                        best_candidates.append((x, y, confidence, matches))
        
        if len(best_candidates) == 0:
            print("–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        best_candidates.sort(key=lambda x: x[2])
        best_candidates = best_candidates[:3]  # –¢–æ–ø-3
        
        # –£—Ä–æ–≤–µ–Ω—å 2: –£—Ç–æ—á–Ω–µ–Ω–∏–µ –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—è—Ö –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        print(f"–£—Ä–æ–≤–µ–Ω—å 2: –£—Ç–æ—á–Ω–µ–Ω–∏–µ {len(best_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
        refined_candidates = []
        
        for candidate_x, candidate_y, _, matches in best_candidates:
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç—å —Å –º–µ–Ω—å—à–∏–º —à–∞–≥–æ–º
            refine_step = search_step // 2
            
            for y in range(max(0, candidate_y - refine_step), 
                          min(map_h - small_h, candidate_y + refine_step), 
                          refine_step // 2):
                for x in range(max(0, candidate_x - refine_step),
                              min(map_w - small_w, candidate_x + refine_step),
                              refine_step // 2):
                    
                    region = large_map[y:min(y + small_h, map_h),
                                     x:min(x + small_w, map_w)]
                    
                    region_objects = self.detect_objects(region)
                    if len(region_objects) > 0:
                        for obj in region_objects:
                            obj.bbox = (obj.bbox[0] + x, obj.bbox[1] + y,
                                       obj.bbox[2] + x, obj.bbox[3] + y)
                            obj.center = (obj.center[0] + x, obj.center[1] + y)
                        
                        new_matches = self._find_object_matches(small_objects, region_objects, top_k=5)
                        if len(new_matches) > 0:
                            confidence = sum(m.distance for m in new_matches) / len(new_matches)
                            refined_candidates.append((x, y, confidence, new_matches))
        
        if len(refined_candidates) == 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≥—Ä—É–±–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            best_x, best_y, _, matches = best_candidates[0]
        else:
            refined_candidates.sort(key=lambda x: x[2])
            best_x, best_y, _, matches = refined_candidates[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
        result = self._estimate_position(matches, large_map.shape, small_h, small_w)
        if result:
            result['x'] = best_x + small_w // 2
            result['y'] = best_y + small_h // 2
        
        return result
    
    def _find_object_matches(self, small_objects: List[ObjectDescriptor],
                           large_objects: List[ObjectDescriptor],
                           top_k: int) -> List:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –º–∞–ª—ã–º –∏ –±–æ–ª—å—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (small_obj, large_obj, distance)
        """
        matches = []
        
        for small_obj in small_objects:
            for large_obj in large_objects:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞
                if small_obj.class_id != large_obj.class_id:
                    continue
                
                distance = small_obj.distance(large_obj)
                matches.append((small_obj, large_obj, distance))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ –±–µ—Ä–µ–º —Ç–æ–ø-k
        matches.sort(key=lambda x: x[2])
        
        return matches[:top_k] if top_k > 0 else matches[:10]
    
    def _estimate_position(self, matches: List, map_shape: Tuple[int, int],
                          small_h: int, small_w: int) -> Optional[Dict]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤
        
        Args:
            matches: –°–ø–∏—Å–æ–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            map_shape: –†–∞–∑–º–µ—Ä—ã –∫–∞—Ä—Ç—ã
            small_h, small_w: –†–∞–∑–º–µ—Ä—ã –º–∞–ª–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
        """
        if len(matches) == 0:
            return None
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º
        distances = [m[2] for m in matches]
        weights = [1 / (d + 0.1) for d in distances]  # –û–±—Ä–∞—Ç–Ω—ã–µ –≤–µ—Å–∞
        total_weight = sum(weights)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ü–µ–Ω—Ç—Ä–æ–≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        center_x = sum(large_obj.center[0] * w for _, large_obj, w in zip(matches, [m[1] for m in matches], weights)) / total_weight
        center_y = sum(large_obj.center[1] * w for _, large_obj, w in zip(matches, [m[1] for m in matches], weights)) / total_weight
        
        # –°–º–µ—â–∞–µ–º —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ –º–∞–ª–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        center_x = center_x - small_w // 2
        center_y = center_y - small_h // 2
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ –∫–∞—Ä—Ç—ã
        center_x = max(small_w // 2, min(map_shape[1] - small_w // 2, center_x))
        center_y = max(small_h // 2, min(map_shape[0] - small_h // 2, center_y))
        
        # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        avg_distance = sum(distances) / len(distances)
        confidence = max(0, (1 - avg_distance) * 100)
        
        # –°–æ–∑–¥–∞–µ–º —É–≥–ª—ã –æ–±–ª–∞—Å—Ç–∏
        corners = [
            [int(center_x - small_w // 2), int(center_y - small_h // 2)],
            [int(center_x + small_w // 2), int(center_y - small_h // 2)],
            [int(center_x + small_w // 2), int(center_y + small_h // 2)],
            [int(center_x - small_w // 2), int(center_y + small_h // 2)]
        ]
        
        return {
            'x': int(center_x),
            'y': int(center_y),
            'angle': 0.0,  # –£–≥–æ–ª –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–º –º–µ—Ç–æ–¥–µ
            'confidence': float(confidence),
            'matches_count': len(matches),
            'corners': corners,
            'detected_objects': len(matches)
        }

